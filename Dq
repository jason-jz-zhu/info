from pyspark.sql import functions as F

# ---- 1. Basic Info ----
df.printSchema()
df.cache()
total_rows = df.count()
print(f"Total records: {total_rows:,}")

# ---- 2. Null Check ----
null_summary = df.select([
    F.sum(F.col(c).isNull().cast("int")).alias(c)
    for c in df.columns
])
print("‚ö†Ô∏è Null value counts per column:")
null_summary.show(truncate=False)

# ---- 3. Duplicate Check ----
dup_count = df.count() - df.dropDuplicates(["digital_profile_id"]).count()
print(f"Duplicate profile IDs: {dup_count}")

# ---- 4. Range Validation (Negative / Outlier) ----
# Identify numeric columns
numeric_cols = [f.name for f in df.schema.fields if f.dataType.simpleString() in ["int", "integer", "float", "double"]]

# Check negative or extreme outliers
range_issues = df.select(
    [F.sum((F.col(c) < 0).cast("int")).alias(c + "_negatives") for c in numeric_cols]
)
print("üö´ Negative value counts:")
range_issues.show(truncate=False)

# Detect potential outliers (e.g., values above 99th percentile)
for c in numeric_cols:
    quantiles = df.approxQuantile(c, [0.99], 0.05)
    if quantiles and quantiles[0] > 0:
        df.filter(F.col(c) > quantiles[0]).select(c).show(3, truncate=False)
        print(f"Top outlier threshold (99th percentile) for {c}: {quantiles[0]}")

# ---- 5. Consistency Across Time Windows ----
# e.g., 90-day >= 30-day >= 7-day >= 1-day
consistency_checks = []
for prefix in ["business_homepage_visits", "business_card_site_visits", "spark_business_site_visits"]:
    cols = [c for c in df.columns if c.startswith(prefix)]
    if len(cols) >= 4:
        c1, c7, c30, c90 = sorted(cols, key=lambda x: int(x.split("_")[-2].replace("last", "").replace("day", "").replace("days", "").replace("month", "30")))
        condition = (F.col(c90) >= F.col(c30)) & (F.col(c30) >= F.col(c7)) & (F.col(c7) >= F.col(c1))
        inconsistent = df.filter(~condition)
        if inconsistent.count() > 0:
            print(f"‚ùó Inconsistent trend detected in: {prefix}")
            inconsistent.select("digital_profile_id", c1, c7, c30, c90).show(5)

# ---- 6. Date Validation ----
date_col = "consumer_platform_last_login_date"
if date_col in df.columns:
    invalid_date = df.filter(F.col(date_col) > F.current_date())
    print("üö® Future dates found in login date:")
    invalid_date.show(5)

# ---- 7. Enum Validation ----
channel_col = "consumer_platform_last_login_channel"
if channel_col in df.columns:
    print("üéØ Distinct login channels:")
    df.select(channel_col).distinct().show()

# ---- 8. Completeness Score ----
non_null_counts = df.select([F.count(F.col(c)).alias(c) for c in df.columns]).collect()[0].asDict()
completeness = {k: round(v / total_rows, 3) for k, v in non_null_counts.items()}
print("üìä Data completeness ratio (non-null rate):")
print(completeness)
